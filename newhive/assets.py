from os.path import dirname, join, abspath, normpath
import os
import json
from boto.s3.key import Key as S3Key
from wsgiref.handlers import format_date_time
from newhive import state, config
from newhive.manage import git
from newhive.utils import *
from md5 import md5


db = state.Database(config)


class Assets:
    def __init__(self, asset_path):
        self.base_path = normpath(join(config.src_home, asset_path))
        self.assets = {}
        bucket_url = db.asset_bucket.generate_url(0)
        self.base_url = bucket_url[0:bucket_url.index('?')]


    # return (path, name) tuples
    def find(self, recurse=True, *opt_start_path):
        start_path = lget(opt_start_path, 0, '')
        strip = len(self.base_path) + 1
        
        for dirname, subdirs, filenames in os.walk(join(self.base_path, start_path)):
            if not recurse: subdirs[:] = [] # slice prevents subdirs being clobbered with new list
            for n in filenames:
                path = join(dirname, n)
                name = path[strip:]
                with open(path) as f: version = md5(f.read()).hexdigest()[:8]
                self.assets[name] = (path, version)

        return self

    # upload each asset
    def push_s3(self):
        # assets expire 10 years from now (we rely on cache busting query string)
        now_stamp = int(now())
        expires = format_date_time(now() + 86400 * 3650)

        versions_key = db.asset_bucket.get_key('.versions.json')
        old_versions = json.loads(versions_key.get_contents_as_string()) if versions_key else {}

        for name, (path, version) in self.assets.iteritems():
            if version != old_versions.get(name):
                print 'uploading: '+ name
                k = S3Key(db.asset_bucket)
                k.name = name
                k.set_contents_from_filename(path, headers={'Expires': expires})
                k.make_public()

        new_versions = dict([(r[0], r[1][1]) for r in self.assets.iteritems()]) # make name: version dict
        if not versions_key: versions_key = S3Key(db.asset_bucket) # create new .versions if it doesn't exist
        versions_key.set_contents_from_string(json.dumps(new_versions))

        return self

    def url(self, name):
        props = self.assets.get(name)
        # TODO: return path of special logging 404 page if asset not found
        return self.base_url + name + '?' + props[1] if props else '/not_found: ' + name

    def write_ruby(self, write_path):
        with open(join(config.src_home, write_path), 'w') as f:
            f.write(
                  '# Hey!\n'
                + '# This file is automatically generated by newhive.assets.Assets.write_ruby on server start\n\n'
                + 'Paths = {\n'
            )

            for name in self.assets:
                f.write('    "'+ name + '" => "' + self.url(name) + '",\n')

            f.write('}')
